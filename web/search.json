[
  {
    "objectID": "imagenet.html",
    "href": "imagenet.html",
    "title": "ImageNet",
    "section": "",
    "text": "To train a diffusion model to generate images conditioned on activations, we need to extract those activations from some model that is looking at some dataset. I have a trained GoogLeNet look at ImageNet images, but due to compute constraints, I could not have GoogLeNet look at all ImageNet images, but only at the first 17760 (which is about 1% of the entire dataset). Here I look at whether that is a problem.\nCode\nNUM_DATA = 17760\nBATCH_SIZE = 64\n\nds = DataLoader(\n    ImageNet('res/imagenet/train'),\n    shuffle=False,\n    batch_size=BATCH_SIZE\n)\nlen(ds)\nNUM_BATCHES = NUM_DATA // BATCH_SIZE\nlabel_count = np.zeros(1000)\nfor i, (images, labels) in zip(range(NUM_BATCHES), ds):\n    for label in labels:\n        label_count[label] += 1\n    if i % (NUM_BATCHES // 10) == 0:\n        print(f\".\", end='', flush=True)\nprint()\n\nexpected_mean = NUM_DATA / 1000\nlabel_count.min(), label_count.max(), label_count.mean(), expected_mean\n\n...........\n\n\n(5.0, 33.0, 17.728, 17.76)\nCode\nfig, ax = plt.subplots()\nax.plot(np.arange(1000), sorted(100 * label_count / NUM_DATA))\nax.plot([0, 999], [0.1, 0.1])\nax.set_ylim(bottom=0)\nax.set_xlim(left=0, right=1000)\nax.set_title(\"label balance of the sampled dataset\")\nax.set_xticks([])\nax.set_xlabel(\"labels ordered by frequency\")\nax.set_ylabel(\"label frequency\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1\nThe sample of the dataset seems reasonably balanced. But a few labels are over- or underrepresented and only by a magnitude of 2 from the mean."
  },
  {
    "objectID": "imagenet.html#image-compression",
    "href": "imagenet.html#image-compression",
    "title": "ImageNet",
    "section": "Image Compression",
    "text": "Image Compression\nLet’s see how much smaller we can make the images without losing recognisabillity.\n\nsample_images, sample_labels = next(iter(ds))\ntuple(sample_images.shape), sample_labels.shape[0]\n\n((64, 3, 224, 224), 64)\n\n\nNot all images are of the same original size, or have all 3 colors. I used the same preprocessing steps as the pytorch implementation to have all images be in the same shape before passing them on to GoogLeNet.\nWe need to undo some of those steps, in order for matplotlib to present the image well.\n\nfrom torchvision.transforms import Resize\n\ndef prepare_image(img, new_size=224, min_=-2.1008, max_=2.6400):\n    img = Resize([new_size, new_size], antialias=True)(img)\n    # channels, height, width  -&gt;  height, width, channels\n    img = img.permute(1, 2, 0)\n    # have the image's values range between 0 and 1\n    img = (img - min_) / (max_ - min_)\n    return img\n\nfig, axes = plt.subplots(2,2)\nfor i, ax in enumerate(axes.flat):\n    image = prepare_image(sample_images[i])\n    ax.axis('off')\n    ax.imshow(image)\n\nplt.suptitle(\"images as seen by GoogLeNet: 224x224\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axes = plt.subplots(2,2)\nfor i, ax in enumerate(axes.flat):\n    image = prepare_image(sample_images[i], 128)\n    ax.axis('off')\n    ax.imshow(image)\n\nplt.suptitle(\"rescaled to 128x128\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axes = plt.subplots(2,2)\nfor i, ax in enumerate(axes.flat):\n    image = prepare_image(sample_images[i], 64)\n    ax.axis('off')\n    ax.imshow(image)\n\nplt.suptitle(\"TinyImageNet: 64x64\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axes = plt.subplots(2,2)\nfor i, ax in enumerate(axes.flat):\n    image = prepare_image(sample_images[i], 28)\n    ax.axis('off')\n    ax.imshow(image)\n\nplt.suptitle(\"MNIST: 28x28\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe 64x64 image seems to still be recognisable enough for a human. For testing purposes I will go with the 28x28 and probably go for 64x64 when training the Conditional Diffusion Model."
  }
]